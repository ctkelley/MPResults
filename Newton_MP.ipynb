{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\calf}{{\\cal F}}\n",
    "\\newcommand{\\dnu}{d \\nu}\n",
    "\\newcommand{\\mf}{{\\bf F}}\n",
    "\\newcommand{\\vu}{{\\bf u}}\n",
    "\\newcommand{\\ve}{{\\bf e}}\n",
    "\\newcommand{\\mg}{{\\bf G}}\n",
    "\\newcommand{\\ml}{{\\bf L}}\n",
    "\\newcommand{\\mg}{{\\bf G}}\n",
    "\\newcommand{\\mi}{{\\bf I}}\n",
    "\\newcommand{\\diag}{\\mbox{diag}}\n",
    "\\newcommand{\\begeq}{{\\begin{equation}}}\n",
    "\\newcommand{\\endeq}{{\\end{equation}}}\n",
    "$\n",
    "\n",
    "\n",
    "# Newton's Method in Multiple Precision: C. T. Kelley\n",
    "\n",
    "This document has two purposes.\n",
    "\n",
    "1. To document the SIREV paper <cite data-cite=\"ctk:sirev19\"><a href=\"newtonmp.html#ctk:sirev19\">(Kel19)</cite>\n",
    "\n",
    "2. To show the RTG people how to do some non-trivial work in Julia and put some LaTeX in their notebooks.\n",
    "\n",
    "As an example we will solve the Chandrasekhar H-equation <cite data-cite=\"chand\"><a href=\"newtonmp.html#chand\">(Cha60)</cite>. This equation, which we describe in detail in the Example section, has a fast $O(N \\log(N))$ function evaluation, a Jacobian evaluation that is $O(N^2)$ work analytically and $O(N^2 \\log(N))$ with a finite difference. This means that most of the work, if you do things right, is in the LU factorization of the Jacobian. \n",
    "    \n",
    "The difference between double, single, and half precision will be clear in the results from the examples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Chandresekhar H-Equation\n",
    "\n",
    "The example is the mid-point rule discretization of the Chandrsekhar H-equation <cite data-cite=\"chand\"><a href=\"newtonmp.html#chand\">(Cha60)</cite>.\n",
    "    \n",
    "    \n",
    " \n",
    "   \\begin{equation}\n",
    "    ({\\calf})(\\mu) = H(\\mu) -\n",
    "\\left(\n",
    "1 - \\frac{c}{2} \\int_0^1 \\frac{\\mu H(\\mu)}{\\mu + \\nu} \\dnu\n",
    "\\right)^{-1} = 0.\n",
    "    \\end{equation}\n",
    "    \n",
    "    \n",
    "The nonlinear operator $\\calf$ is defined on $C[0,1]$, the space of\n",
    "continuous functions on $[0,1]$.  \n",
    "\n",
    "The equation has a well-understood dependence on the parameter $c$\n",
    "<cite data-cite=\"twm68\"><a href=\"newtonmp.html#twm68\">(Mul68)</cite>,\n",
    "    <cite data-cite=\"ctk:n1\"><a href=\"newtonmp.html#ctk:n1\">(DK80)</cite>.\n",
    "The equation has unique solutions at $c=0$\n",
    "and $c=1$ and two solutions for $0 < c < 1$. There is a simple fold\n",
    "singularity \n",
    "        <cite data-cite=\"herb\"><a href=\"newtonmp.html#herb\">(Kel87)</cite>\n",
    "        at $c=1$. Only one \n",
    "        <cite data-cite=\"chand\"><a href=\"newtonmp.html#chand\">(Cha60)</cite>,\n",
    "        <cite data-cite=\"busb\"><a href=\"newtonmp.html#busb\"(Bus60)</cite>\n",
    "            \n",
    "of the two solutions for $0 < c < 1$ is of physical interest\n",
    "and that is the one easiest to find numerically. One must do\n",
    "a continuation computation to find the other one.\n",
    "\n",
    "The structure of the singularity is preserved if one discretizes\n",
    "the integral with any rule that integrates constants exactly. For\n",
    "the purposes of this paper the composite midpoint rule will suffice.\n",
    "The $N$-point composite midpoint rule is\n",
    "\\begin{equation}\n",
    "\\int_0^1 f(\\nu) \\dnu \\approx \\frac{1}{N} \\sum_{j=1}^N f(\\nu_j)\n",
    "\\end{equation}\n",
    "where $\\nu_j = (j - 1/2)/N$ for $1 \\le j \\le N$. This rule is\n",
    "second-order accurate for sufficiently smooth functions $f$. The\n",
    "solution of the integral equation is, however, not smooth enough. $H'(\\mu)$\n",
    "has a logarithmic singularity at $\\mu=0$.\n",
    "\n",
    "The discrete problem is\n",
    "\n",
    "\\begin{equation}\n",
    "\\mf(\\vu)_i \\equiv\n",
    "u_i - \\left(\n",
    "1  - \\frac{c}{2N} \\sum_{j=1}^N \\frac{u_j \\mu_i}{\\mu_j + \\mu_i}\n",
    "\\right)^{-1}\n",
    "=0.\n",
    "\\end{equation}\n",
    "\n",
    "One can simplify the approximate integral operator in \\eqnok{hmid}\n",
    "and expose some useful structure. Since\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{c}{2N} \\sum_{j=1}^N \\frac{u_j \\mu_i}{\\mu_j + \\mu_i}\n",
    "= \\frac{c (i - 1/2) }{2N} \\sum_{j=1}^N \\frac{u_j}{i+j -1}.\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "hence the approximate integral operator is\n",
    "the product of a diagonal matrix and a Hankel matrix and\n",
    "one can use an FFT to evaluate that operator with $O(N \\log(N))$\n",
    "work \n",
    "<cite data-cite=\"golub\"><a href=\"newtonmp.html#golub\">(GV96)</cite>.\n",
    "    \n",
    "We can express the approximation of the integral operator in matrix\n",
    "form\n",
    "\\begin{equation}\n",
    "\\ml(\\vu)_i = \\frac{c (i - 1/2) }{2N} \\sum_{j=1}^N \\frac{u_j}{i+j -1}\n",
    "\\end{equation}\n",
    "and compute the Jacobian analytically as\n",
    "\\begin{equation}\n",
    "\\mf'(\\vu) = \\mi - \\diag(\\mg(\\vu))^2 \\ml\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation}\n",
    "\\mg(\\vu)_i = \\left(\n",
    "1  - \\frac{c}{2N} \\sum_{j=1}^N \\frac{u_j \\mu_i}{\\mu_j + \\mu_i}\n",
    "\\right)^{-1}.\n",
    "\\end{equation}\n",
    "Hence the data for the Jacobian is already available after\n",
    "one computes $\\mf(\\vu) = \\vu - \\mg(\\vu)$ and the Jacobian can\n",
    "be computed with $O(N^2)$ work.\n",
    "We do that in this example and therefore the only $O(N^3)$\n",
    "part of the solve is the matrix factorization.\n",
    "\n",
    "One could also approximate the Jacobian with forward differences.\n",
    "In this case one approximates the $j$th column $\\mf'(\\vu)_j$\n",
    "of the Jacobian with\n",
    "\\begin{equation}\n",
    "\\frac{\\mf(\\vu + h {\\tilde \\ve}_j) - \\mf(\\vu)}{h}\n",
    "\\end{equation}\n",
    "where ${\\tilde \\ve}_j$ is a unit vector in the $j$th coordinate\n",
    "direction and $h$ is a suitable difference increment. If one computes\n",
    "$\\mf$ in double precision with unit roundoff $u_d$, then\n",
    "$h =O(\\| \\vu \\| \\sqrt{u_d})$ is a reasonable choice \\cite{ctk:roots}. Then\n",
    "the error in the Jacobian is $O(\\sqrt{u_d}) = O(u_s)$ where $u_s$ is\n",
    "unit roundoff in single precision. The cost of a finite difference Jacobian\n",
    "in this example is $O(N^2 \\log(N))$ work.\n",
    "\n",
    "The analysis in  <cite data-cite=\"ctk:sirev19\"><a href=\"newtonmp.html#ctk:sirev19\">(Kel19)</cite> uggests that there\n",
    "is no significant difference in the nonlinear iteration\n",
    "from either the choice of analytic or finite difference Jacobians\n",
    "or the choice of single or double precision for the linear solver. This notebook has the data used in that paper\n",
    "to support that assertion. You will be able to duplicate the results and play with the codes.\n",
    "    \n",
    "Half precision is another story and we have those codes for you, too.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up\n",
    "\n",
    "You need to install these packages:\n",
    "\n",
    "- PyPlot\n",
    "- LinearAlgebra\n",
    "- JLD2\n",
    "- Printf\n",
    "- FFTW\n",
    "- IJulia (You must have done this already or you would not be looking at this notebook.)\n",
    "- AbstractFFTs\n",
    "\n",
    "The directory is a Julia project. So all you should need to do to get going is to\n",
    "\n",
    "1. Put the directory in your LOAD_PATH. The way to do this is to type\n",
    "```Julia\n",
    "push!(LOAD_PATH,\"/Users/yourid/whereyouputit/MPResults2019\")\n",
    "```\n",
    "at the Julia prompt in the REPL or in a notebook code windown.\n",
    "\n",
    "2. Now load the modules with\n",
    "```Julia\n",
    "using MPResults2019\n",
    "```\n",
    "\n",
    "3. Then you can do a simple solve and test that you did it right by typing\n",
    "```Julia\n",
    "hout=heqtest()\n",
    "```\n",
    "which I will do now. Make sure __MPResults2019__ is in your LOAD_PATH! If you forget to do the push! command, strange things may happen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Recompiling stale cache file /Users/ctk/.julia/compiled/v1.1/MPResults2019/NvegU.ji for MPResults2019 [242f216b-ff5d-5b0f-bbbe-27950a0da07d]\n",
      "└ @ Base loading.jl:1184\n"
     ]
    }
   ],
   "source": [
    "push!(LOAD_PATH,\"/Users/ctk/Dropbox/Julia/MPResults2019\")\n",
    "using MPResults2019\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00e+00      1.00000e+00 \n",
      "5.00e-02      1.04424e+00 \n",
      "1.00e-01      1.07236e+00 \n",
      "1.50e-01      1.09470e+00 \n",
      "2.00e-01      1.11346e+00 \n",
      "2.50e-01      1.12965e+00 \n",
      "3.00e-01      1.14389e+00 \n",
      "3.50e-01      1.15657e+00 \n",
      "4.00e-01      1.16797e+00 \n",
      "4.50e-01      1.17830e+00 \n",
      "5.00e-01      1.18773e+00 \n",
      "5.50e-01      1.19638e+00 \n",
      "6.00e-01      1.20435e+00 \n",
      "6.50e-01      1.21172e+00 \n",
      "7.00e-01      1.21856e+00 \n",
      "7.50e-01      1.22493e+00 \n",
      "8.00e-01      1.23088e+00 \n",
      "8.50e-01      1.23646e+00 \n",
      "9.00e-01      1.24169e+00 \n",
      "9.50e-01      1.24662e+00 \n",
      "1.00e+00      1.25126e+00 \n",
      "\n",
      " \n",
      " \n",
      "[1.54457e+00, 7.94167e-03, 1.55209e-07, 2.39149e-15, 1.61651e-15, 1.36877e-15, 1.47288e-15, 1.76242e-15, 2.20932e-15, 1.50598e-15, 1.06489e-15]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(exactout = (solution = [1.00707e+00; 1.01754e+00; … ; 1.24989e+00; 1.25081e+00], ithist = [1.54457e+00, 7.94167e-03, 1.55209e-07, 2.39149e-15, 1.61651e-15, 1.36877e-15, 1.47288e-15, 1.76242e-15, 2.20932e-15, 1.50598e-15, 1.06489e-15]), fdout = (solution = [1.00707e+00; 1.01754e+00; … ; 1.24989e+00; 1.25081e+00], ithist = [1.54457e+00, 7.94166e-03, 1.55234e-07, 2.02292e-15, 1.40433e-15, 1.29473e-15, 1.47288e-15, 1.52226e-15, 1.27555e-15, 1.43901e-15, 1.38667e-15]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heqtest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "heqtest.jl, calls the solver and harvests some iteration statistics. The two columns of numbers are the reults from <cite data-cite=\"chand\"><a href=\"newtonmp.html#chand\">(Cha60)</cite> (page 125). The iteration statistics are from KNL, the solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/Users/ctk/Dropbox/Julia/MPResults2019/Mixed_Precision_c=5\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Home4mp=pwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  KNL\n",
    "\n",
    "The solver is _knl.jl_ version .01. Keep in mind that nothing with a version number with a negative exponent field is likely to be very good. knl.jl is included when you run the MPResults.jl module, which you do automatically when you\n",
    "type __using MPResults2019__\n",
    "\n",
    "\n",
    "knl.jl is a simple implemention of Newton's method (see \n",
    "<cite data-cite=\"ctk:roots\"><a href=\"newtonmp.html#ctk:roots\">(Kel95)</cite> and\n",
    "<cite data-cite=\"ctk:newton\"><a href=\"newtonmp.html#ctk:newton\">(Kel03)</cite> )\n",
    "using an LU factorization of the Jacobian to compute the Newton step with no line search or globalization. The code evaluates and factors the Jacobian at every nonlinear iteration.\n",
    "\n",
    "Compare this with the scalar code with talked about last time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using knl.jl ##\n",
    "\n",
    "At the level of this notebook, it's pretty simple. Remember that Julia hates to allocate mememory. So your function and Jacobian evaluation routines should expect the calling function to **preallocate** the storage for both the function and Jacobian. Your functions will then use __.=__ to put the function and Jacobian where they are supposed to be.\n",
    "\n",
    "It's worthwhile to look at the help screen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth asking for help with knl ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mk\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ml\u001b[22m plot\u001b[0m\u001b[1mk\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ml\u001b[22m install\u001b[0m\u001b[1mk\u001b[22mer\u001b[0m\u001b[1mn\u001b[22me\u001b[0m\u001b[1ml\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "knl(x, FS, FPS, F!, J!=diffjac!; rtol=1.e-6, atol=1.e-12, \n",
       "        maxit=20, dx=1.e-6, pdata=nothing)\n",
       "\\end{verbatim}\n",
       "This is Version .01. Nothing with a version number having a negative exponent field can be trusted.\n",
       "\n",
       "Nonlinear solvers from my books in Julia. This version has no globalization, no quasi-Newton methods, and no Newton-Krylov. The mission here is to  duplicate the mixed precision results in my SIREV-ED submission.\n",
       "\n",
       "\\section{Inputs:}\n",
       "\\begin{itemize}\n",
       "\\item x: initial iterate\n",
       "\n",
       "\n",
       "\\item FS: Preallcoated storage for function. It is an N x 1 column vector\n",
       "\n",
       "\n",
       "\\item FPS: preallcoated storage for Jacobian. It is an N x N matrix\n",
       "\n",
       "\n",
       "\\item F!: function evaluation, the ! indicates that F! overwrites FS, your   preallocated storage for the function.\n",
       "\n",
       "\n",
       "\\item J!: Jacobian evaluation, the ! indicates that J! overwrites FPS, your   preallocated storage for the Jacobian. If you leave this out the   default is a finite difference Jacobian.\n",
       "\n",
       "\\end{itemize}\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\section{keyword arguments (kwargs):}\n",
       "\\begin{itemize}\n",
       "\\item rtol and atol: relative and absolute error tolerances\n",
       "\n",
       "\n",
       "\\item maxit: limit on nonlinear iterations\n",
       "\n",
       "\n",
       "\\item dx: difference increment in finite-difference derivatives     h=dx*norm(x)+1.e-8\n",
       "\n",
       "\n",
       "\\item pdata: precomputed data for the function/Jacobian.       Things will go better if you use this rather than hide the data       in global variables within the module for your function/Jacobian\n",
       "\n",
       "\\end{itemize}\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\section{Using knl.jl}\n",
       "Here are the rules as of June 6, 2019\n",
       "\n",
       "F! is the nonlinear residual.  J! is the Jacobian evaluation.\n",
       "\n",
       "Put these things in a module and cook it up so that\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item[1. ] You allocate storage for the function and Jacobian in advance  –> in the calling program <– NOT in FS and FPS\n",
       "\n",
       "\\end{itemize}\n",
       "FV=F!(FV,x) returns FV=F(x)\n",
       "\n",
       "JV=J!(FV,FP,x) returns FP=F'(x);      (FV,FP, x) must be the argument list, even if J! does not need FP.     One reason for this is that the finite-difference Jacobian     does and that is the default in the solver.\n",
       "\n",
       "In the futrue J! will also be a matrix-vector product and FPS will be the PREALLOCATED (!!) storage for the GMRES(m) Krylov vectors.\n",
       "\n",
       "Lemme tell ya 'bout precision. I designed this code for full precision functions and linear algebra in any precision you want. You can decleare FPS as Float64, Float32, or Float16 and knl will do the right thing if  YOU do not destroy the declaration in your J! function. I'm amazed  that this works so easily. \n",
       "\n",
       "If the Jacobian is reasonably well conditioned, I can see no reason to do linear algebra in double precision\n",
       "\n",
       "Don't try to evaluate function and Jacobian all at once because  that will cost you a extra function evaluation everytime the line search kicks in.\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item[2. ] Any precomputed data for functions, Jacobians, matrix-vector products may live in global variables within the module. Don't do that if you can avoid it. Use pdata instead.\n",
       "\n",
       "\\end{itemize}\n"
      ],
      "text/markdown": [
       "```\n",
       "knl(x, FS, FPS, F!, J!=diffjac!; rtol=1.e-6, atol=1.e-12, \n",
       "        maxit=20, dx=1.e-6, pdata=nothing)\n",
       "```\n",
       "\n",
       "This is Version .01. Nothing with a version number having a negative exponent field can be trusted.\n",
       "\n",
       "Nonlinear solvers from my books in Julia. This version has no globalization, no quasi-Newton methods, and no Newton-Krylov. The mission here is to  duplicate the mixed precision results in my SIREV-ED submission.\n",
       "\n",
       "# Inputs:\n",
       "\n",
       "  * x: initial iterate\n",
       "  * FS: Preallcoated storage for function. It is an N x 1 column vector\n",
       "  * FPS: preallcoated storage for Jacobian. It is an N x N matrix\n",
       "  * F!: function evaluation, the ! indicates that F! overwrites FS, your   preallocated storage for the function.\n",
       "  * J!: Jacobian evaluation, the ! indicates that J! overwrites FPS, your   preallocated storage for the Jacobian. If you leave this out the   default is a finite difference Jacobian.\n",
       "\n",
       "---\n",
       "\n",
       "# keyword arguments (kwargs):\n",
       "\n",
       "  * rtol and atol: relative and absolute error tolerances\n",
       "  * maxit: limit on nonlinear iterations\n",
       "  * dx: difference increment in finite-difference derivatives     h=dx*norm(x)+1.e-8\n",
       "  * pdata: precomputed data for the function/Jacobian.       Things will go better if you use this rather than hide the data       in global variables within the module for your function/Jacobian\n",
       "\n",
       "---\n",
       "\n",
       "# Using knl.jl\n",
       "\n",
       "Here are the rules as of June 6, 2019\n",
       "\n",
       "F! is the nonlinear residual.  J! is the Jacobian evaluation.\n",
       "\n",
       "Put these things in a module and cook it up so that\n",
       "\n",
       "1. You allocate storage for the function and Jacobian in advance  –> in the calling program <– NOT in FS and FPS\n",
       "\n",
       "FV=F!(FV,x) returns FV=F(x)\n",
       "\n",
       "JV=J!(FV,FP,x) returns FP=F'(x);      (FV,FP, x) must be the argument list, even if J! does not need FP.     One reason for this is that the finite-difference Jacobian     does and that is the default in the solver.\n",
       "\n",
       "In the futrue J! will also be a matrix-vector product and FPS will be the PREALLOCATED (!!) storage for the GMRES(m) Krylov vectors.\n",
       "\n",
       "Lemme tell ya 'bout precision. I designed this code for full precision functions and linear algebra in any precision you want. You can decleare FPS as Float64, Float32, or Float16 and knl will do the right thing if  YOU do not destroy the declaration in your J! function. I'm amazed  that this works so easily. \n",
       "\n",
       "If the Jacobian is reasonably well conditioned, I can see no reason to do linear algebra in double precision\n",
       "\n",
       "Don't try to evaluate function and Jacobian all at once because  that will cost you a extra function evaluation everytime the line search kicks in.\n",
       "\n",
       "2. Any precomputed data for functions, Jacobians, matrix-vector products may live in global variables within the module. Don't do that if you can avoid it. Use pdata instead.\n"
      ],
      "text/plain": [
       "\u001b[36m  knl(x, FS, FPS, F!, J!=diffjac!; rtol=1.e-6, atol=1.e-12, \u001b[39m\n",
       "\u001b[36m          maxit=20, dx=1.e-6, pdata=nothing)\u001b[39m\n",
       "\n",
       "  This is Version .01. Nothing with a version number having a negative\n",
       "  exponent field can be trusted.\n",
       "\n",
       "  Nonlinear solvers from my books in Julia. This version has no globalization,\n",
       "  no quasi-Newton methods, and no Newton-Krylov. The mission here is to\n",
       "  duplicate the mixed precision results in my SIREV-ED submission.\n",
       "\n",
       "\u001b[1m  Inputs:\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "    •    x: initial iterate\n",
       "\n",
       "    •    FS: Preallcoated storage for function. It is an N x 1 column\n",
       "        vector\n",
       "\n",
       "    •    FPS: preallcoated storage for Jacobian. It is an N x N matrix\n",
       "\n",
       "    •    F!: function evaluation, the ! indicates that F! overwrites FS,\n",
       "        your preallocated storage for the function.\n",
       "\n",
       "    •    J!: Jacobian evaluation, the ! indicates that J! overwrites FPS,\n",
       "        your preallocated storage for the Jacobian. If you leave this out\n",
       "        the default is a finite difference Jacobian.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[1m  keyword arguments (kwargs):\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "    •    rtol and atol: relative and absolute error tolerances\n",
       "\n",
       "    •    maxit: limit on nonlinear iterations\n",
       "\n",
       "    •    dx: difference increment in finite-difference derivatives\n",
       "        h=dx*norm(x)+1.e-8\n",
       "\n",
       "    •    pdata: precomputed data for the function/Jacobian. Things will go\n",
       "        better if you use this rather than hide the data in global\n",
       "        variables within the module for your function/Jacobian\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[1m  Using knl.jl\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "  Here are the rules as of June 6, 2019\n",
       "\n",
       "  F! is the nonlinear residual. J! is the Jacobian evaluation.\n",
       "\n",
       "  Put these things in a module and cook it up so that\n",
       "\n",
       "    1.   You allocate storage for the function and Jacobian in advance –>\n",
       "        in the calling program <– NOT in FS and FPS\n",
       "\n",
       "  FV=F!(FV,x) returns FV=F(x)\n",
       "\n",
       "  JV=J!(FV,FP,x) returns FP=F'(x); (FV,FP, x) must be the argument list, even\n",
       "  if J! does not need FP. One reason for this is that the finite-difference\n",
       "  Jacobian does and that is the default in the solver.\n",
       "\n",
       "  In the futrue J! will also be a matrix-vector product and FPS will be the\n",
       "  PREALLOCATED (!!) storage for the GMRES(m) Krylov vectors.\n",
       "\n",
       "  Lemme tell ya 'bout precision. I designed this code for full precision\n",
       "  functions and linear algebra in any precision you want. You can decleare FPS\n",
       "  as Float64, Float32, or Float16 and knl will do the right thing if YOU do\n",
       "  not destroy the declaration in your J! function. I'm amazed that this works\n",
       "  so easily. \n",
       "\n",
       "  If the Jacobian is reasonably well conditioned, I can see no reason to do\n",
       "  linear algebra in double precision\n",
       "\n",
       "  Don't try to evaluate function and Jacobian all at once because that will\n",
       "  cost you a extra function evaluation everytime the line search kicks in.\n",
       "\n",
       "    2.   Any precomputed data for functions, Jacobians, matrix-vector\n",
       "        products may live in global variables within the module. Don't do\n",
       "        that if you can avoid it. Use pdata instead."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?knl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How knl.jl controls the precision of the Jacobian##\n",
    "\n",
    "You can control the precision of the Jacobian by simply allocating FPS in your favorite precision. So if I have a problem with N=256 unknows I will decalare FP as zeros(N,1) and may delcare FPS as zeros(N,N) or Float32.(zeros(N,N)).\n",
    "\n",
    "Note the __.__ between Float32 and the paren. This, as is standard Julia practice, applies the conversion to Float32 to everyelement in the array. If you forget the __.__ Julia will complain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The results in the paper\n",
    "\n",
    "The paper has plots for double, single, and half precsion computations for c=.5, .99, and 1.0. The half precision results take a very long time to get. On my computer (2019 iMac; 8 cores, 64GB of memoroy) the half precision compute time was over two weeks. Kids don't try this at home.\n",
    "\n",
    "The data for the paper are in the cleverly named directory __Data_From_Paper__\n",
    "\n",
    "__cd to the directory MPResults2019__ and __from that directory__ run\n",
    "\n",
    "```Julia\n",
    "data_harvest(\"Data_From_Paper/MP_Data_\") \n",
    "```\n",
    "\n",
    "at the julia prompt you will generate all the tables and plots in the paper.\n",
    "\n",
    "If you have the time and patience you can also generate the data with data_populate.jl. This create three directories named Mixed_Precision_c=? and you can see for yourself. Look at that file to see opportunities to edit the time-cosumng jobs out. If you eliminate the half precision work and the larger dimensions, the code will run in a short time. \n",
    "\n",
    "Here is a simple example of using data_populate and the plotter code plot_knl.jl. I'm only using the 1024, 2048 and 4096 point grids. The plot in the paper uses more levels. This is part of Figure 1 in the paper.\n",
    "\n",
    "Look at the source to data_populate and plotknl and you'll see how I did this. These codes only mange files, plots, and tables. There is nothing really exciting here. You don't need to know much Julia to understand this, but you do need to know something and I can't help with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/Users/ctk/Dropbox/Julia/MPResults2019\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Home4mp=\"/Users/ctk/Dropbox/Julia/MPResults2019\"\n",
    "cd(Home4mp)\n",
    "pwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd(Home4mp)\n",
    "using PyPlot\n",
    "data_populate(.5;half=\"no\",level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd(Home4mp)\n",
    "cd(\"Mixed_Precision_c=5\")\n",
    "figtitle=\"Figure 1\"\n",
    "plotknl(\"no\",.5,10,3;bigtitle=figtitle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__data_populate__ will create directories for your data if they are not already there. Run __plot_knl__ from one of those directories and it makes the plots. As you can see from the one above, it's hard to tell the difference between double and single precision linear algebra and analytic or finite difference Jacobians. You knew that."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.1",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
