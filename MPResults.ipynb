{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\calf}{{\\cal F}}\n",
    "\\newcommand{\\dnu}{d \\nu}\n",
    "\\newcommand{\\mf}{{\\bf F}}\n",
    "\\newcommand{\\vu}{{\\bf u}}\n",
    "\\newcommand{\\ve}{{\\bf e}}\n",
    "\\newcommand{\\mg}{{\\bf G}}\n",
    "\\newcommand{\\ml}{{\\bf L}}\n",
    "\\newcommand{\\mg}{{\\bf G}}\n",
    "\\newcommand{\\mi}{{\\bf I}}\n",
    "\\newcommand{\\diag}{\\mbox{diag}}\n",
    "\\newcommand{\\begeq}{{\\begin{equation}}}\n",
    "\\newcommand{\\endeq}{{\\end{equation}}}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"mprnote.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newton's Method in Multiple Precision: C. T. Kelley\n",
    "\n",
    "This notebook documents the results in \n",
    "C. T. Kelley, ***Newton's Method in Mixed Precision***, 2020.<cite data-cite=\"ctk:sirev20\"><a href=\"newtonmp.html#ctk:sirev20\">(Kel20)</cite><br>\n",
    "\n",
    "As an example we will solve the Chandrasekhar H-equation <cite data-cite=\"chand\"><a href=\"newtonmp.html#chand\">(Cha60)</cite>. This equation, which we describe in detail in the Example section, has a fast $O(N \\log(N))$ function evaluation, a Jacobian evaluation that is $O(N^2)$ work analytically and $O(N^2 \\log(N))$ with a finite difference. This means that most of the work, if you do things right, is in the LU factorization of the Jacobian. \n",
    "    \n",
    "The difference between double, single, and half precision will be clear in the results from the examples. This notebook has no half-precision computations. Julia does half-precsion in software and that is very slow.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Example: The Chandrasekhar H-Equation](#The-Chandrasekhar-H-Equation): Integral equations example\n",
    "\n",
    "- [Setting up the notebook](#Setting-up): Install the application.\n",
    "\n",
    "- [First run of the solver](#Running-the-solver): First solver teset\n",
    "\n",
    "- [How to use the solver](#NSOL): Using nsol.jl.\n",
    "\n",
    "- [The results in the paper](#The-results-in-the-paper): Running the codes that generated the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Chandrasekhar H-Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "The example is the mid-point rule discretization of the Chandrasekhar H-equation <cite data-cite=\"chand\"><a href=\"newtonmp.html#chand\">(Cha60)</cite>.\n",
    "    \n",
    "    \n",
    " \n",
    "   \\begin{equation}\n",
    "    {\\calf}(H)(\\mu) = H(\\mu) -\n",
    "\\left(\n",
    "1 - \\frac{c}{2} \\int_0^1 \\frac{\\mu H(\\mu)}{\\mu + \\nu} \\dnu\n",
    "\\right)^{-1} = 0.\n",
    "    \\end{equation}\n",
    "    \n",
    "    \n",
    "The nonlinear operator $\\calf$ is defined on $C[0,1]$, the space of\n",
    "continuous functions on $[0,1]$.  \n",
    "\n",
    "The equation has a well-understood dependence on the parameter $c$\n",
    "<cite data-cite=\"twm68\"><a href=\"newtonmp.html#twm68\">(Mul68)</cite>,\n",
    "    <cite data-cite=\"ctk:n1\"><a href=\"newtonmp.html#ctk:n1\">(DK80)</cite>.\n",
    "The equation has unique solutions at $c=0$\n",
    "and $c=1$ and two solutions for $0 < c < 1$. There is a simple fold\n",
    "singularity \n",
    "        <cite data-cite=\"herb\"><a href=\"newtonmp.html#herb\">(Kel87)</cite>\n",
    "        at $c=1$. Only one \n",
    "        <cite data-cite=\"chand\"><a href=\"newtonmp.html#chand\">(Cha60)</cite>,\n",
    "        <cite data-cite=\"busb\"><a href=\"newtonmp.html#busb\">(Bus60)</cite>\n",
    "            of the two solutions for $0 < c < 1$ is of physical interest\n",
    "and that is the one easiest to find numerically. One must do\n",
    "a continuation computation to find the other one.\n",
    "\n",
    "The structure of the singularity is preserved if one discretizes\n",
    "the integral with any rule that integrates constants exactly. For\n",
    "the purposes of this paper the composite midpoint rule will suffice.\n",
    "The $N$-point composite midpoint rule is\n",
    "\\begin{equation}\n",
    "\\int_0^1 f(\\nu) \\dnu \\approx \\frac{1}{N} \\sum_{j=1}^N f(\\nu_j)\n",
    "\\end{equation}\n",
    "where $\\nu_j = (j - 1/2)/N$ for $1 \\le j \\le N$. This rule is\n",
    "second-order accurate for sufficiently smooth functions $f$. The\n",
    "solution of the integral equation is, however, not smooth enough. $H'(\\mu)$\n",
    "has a logarithmic singularity at $\\mu=0$.\n",
    "\n",
    "The discrete problem is\n",
    "\n",
    "\\begin{equation}\n",
    "\\mf(\\vu)_i \\equiv\n",
    "u_i - \\left(\n",
    "1  - \\frac{c}{2N} \\sum_{j=1}^N \\frac{u_j \\mu_i}{\\mu_j + \\mu_i}\n",
    "\\right)^{-1}\n",
    "=0.\n",
    "\\end{equation}\n",
    "\n",
    "One can simplify the approximate integral operator\n",
    "and expose some useful structure. Since\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{c}{2N} \\sum_{j=1}^N \\frac{u_j \\mu_i}{\\mu_j + \\mu_i}\n",
    "= \\frac{c (i - 1/2) }{2N} \\sum_{j=1}^N \\frac{u_j}{i+j -1}.\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "hence the approximate integral operator is\n",
    "the product of a diagonal matrix and a Hankel matrix and\n",
    "one can use an FFT to evaluate that operator with $O(N \\log(N))$\n",
    "work \n",
    "<cite data-cite=\"golub\"><a href=\"newtonmp.html#golub\">(GV96)</cite>.\n",
    "    \n",
    "We can express the approximation of the integral operator in matrix\n",
    "form\n",
    "\\begin{equation}\n",
    "(\\ml \\vu)_i = \\frac{c (i - 1/2) }{2N} \\sum_{j=1}^N \\frac{u_j}{i+j -1}\n",
    "\\end{equation}\n",
    "and compute the Jacobian analytically as\n",
    "\\begin{equation}\n",
    "\\mf'(\\vu) = \\mi - \\diag(\\mg(\\vu))^2 \\ml\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation}\n",
    "\\mg(\\vu)_i = \\left(\n",
    "1  - \\frac{c}{2N} \\sum_{j=1}^N \\frac{u_j \\mu_i}{\\mu_j + \\mu_i}\n",
    "\\right)^{-1}.\n",
    "\\end{equation}\n",
    "Hence the data for the Jacobian is already available after\n",
    "one computes $\\mf(\\vu) = \\vu - \\mg(\\vu)$ and the Jacobian can\n",
    "be computed with $O(N^2)$ work.\n",
    "We do that in this example and therefore the only $O(N^3)$\n",
    "part of the solve is the matrix factorization.\n",
    "\n",
    "One could also approximate the Jacobian with forward differences.\n",
    "In this case one approximates the $j$th column $\\mf'(\\vu)_j$\n",
    "of the Jacobian with\n",
    "\\begin{equation}\n",
    "\\frac{\\mf(\\vu + h {\\tilde \\ve}_j) - \\mf(\\vu)}{h}\n",
    "\\end{equation}\n",
    "where ${\\tilde \\ve}_j$ is a unit vector in the $j$th coordinate\n",
    "direction and $h$ is a suitable difference increment. If one computes\n",
    "$\\mf$ in double precision with unit roundoff $u_d$, then\n",
    "$h =O(\\| \\vu \\| \\sqrt{u_d})$ is a reasonable choice \n",
    "<cite data-cite=\"ctk:roots\"><a href=\"newtonmp.html#ctk:roots\">(Kel95)</cite>.\n",
    "Then\n",
    "the error in the Jacobian is $O(\\sqrt{u_d}) = O(u_s)$ where $u_s$ is\n",
    "unit roundoff in single precision. The cost of a finite difference Jacobian\n",
    "in this example is $O(N^2 \\log(N))$ work.\n",
    "\n",
    "The analysis in  <cite data-cite=\"ctk:sirev20\"><a href=\"newtonmp.html#ctk:sirev20\">(Kel20)</cite> suggests that there\n",
    "is no significant difference in the nonlinear iteration\n",
    "from either the choice of analytic or finite difference Jacobians\n",
    "or the choice of single or double precision for the linear solver. This notebook has the data used in that paper\n",
    "to support that assertion. You will be able to duplicate the results and play with the codes.\n",
    "    \n",
    "Half precision is another story and we have those codes for you, too.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "You need to install these packages with __Pkg__. I assume you know how to do that.\n",
    "\n",
    "- SIAMFANLEquations\n",
    "- PyPlot\n",
    "- LinearAlgebra\n",
    "- Printf\n",
    "- IJulia (You must have done this already or you would not be looking at this notebook.)\n",
    "\n",
    "To render the LaTeX you'll need to run  the first markdown cell in the notebook. That sets up the commands you need to render the LaTeX correctly.\n",
    "\n",
    "The directory is a Julia project. So all you should need to do to get going is to run the first code cell in this notebook. That cell has one line\n",
    "\n",
    "```\n",
    "include(\"mprnote.jl\")\n",
    "```\n",
    "\n",
    "Then you can do a simple solve and test that you did it right by typing\n",
    "```Julia\n",
    "hout=heqtest()\n",
    "```\n",
    "which I will do in the next code cell. Now ...\n",
    "\n",
    "**Make absolutely sure that you are in the MPResults directory**. Then ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The codes solve the H-equation and plot/tabulate the results in various ways. __heqtest__ prints on column of the tables in Chandrasekhar's book. It calls __nsold.jl__ . I've shown you the output from the solver but that is not important for now. You get the details on the solver in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00e+00      1.00000e+00 \n",
      "5.00e-02      1.04424e+00 \n",
      "1.00e-01      1.07236e+00 \n",
      "1.50e-01      1.09470e+00 \n",
      "2.00e-01      1.11346e+00 \n",
      "2.50e-01      1.12965e+00 \n",
      "3.00e-01      1.14389e+00 \n",
      "3.50e-01      1.15657e+00 \n",
      "4.00e-01      1.16797e+00 \n",
      "4.50e-01      1.17830e+00 \n",
      "5.00e-01      1.18773e+00 \n",
      "5.50e-01      1.19638e+00 \n",
      "6.00e-01      1.20435e+00 \n",
      "6.50e-01      1.21172e+00 \n",
      "7.00e-01      1.21856e+00 \n",
      "7.50e-01      1.22493e+00 \n",
      "8.00e-01      1.23088e+00 \n",
      "8.50e-01      1.23646e+00 \n",
      "9.00e-01      1.24169e+00 \n",
      "9.50e-01      1.24662e+00 \n",
      "1.00e+00      1.25126e+00 \n",
      "\n",
      " \n",
      " \n",
      "[1.54457e+00, 7.94167e-03, 1.55209e-07, 2.41202e-15, 1.43901e-15, 1.36877e-15, 1.53837e-15, 1.79018e-15, 2.10650e-15, 1.48952e-15, 1.50598e-15]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(exactout = (solution = [1.00707e+00, 1.01754e+00, 1.02622e+00, 1.03392e+00, 1.04094e+00, 1.04744e+00, 1.05352e+00, 1.05925e+00, 1.06469e+00, 1.06986e+00  …  1.24220e+00, 1.24320e+00, 1.24419e+00, 1.24517e+00, 1.24614e+00, 1.24709e+00, 1.24804e+00, 1.24897e+00, 1.24989e+00, 1.25081e+00], functionval = [0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00  …  0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, -2.22045e-16, 0.00000e+00, 2.22045e-16, -2.22045e-16, -2.22045e-16, 0.00000e+00], history = [1.54457e+00, 7.94167e-03, 1.55209e-07, 2.41202e-15, 1.43901e-15, 1.36877e-15, 1.53837e-15, 1.79018e-15, 2.10650e-15, 1.48952e-15, 1.50598e-15], stats = (ifun = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], ijac = [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], iarm = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), idid = false, errcode = 10), fdout = (solution = [1.00707e+00, 1.01754e+00, 1.02622e+00, 1.03392e+00, 1.04094e+00, 1.04744e+00, 1.05352e+00, 1.05925e+00, 1.06469e+00, 1.06986e+00  …  1.24220e+00, 1.24320e+00, 1.24419e+00, 1.24517e+00, 1.24614e+00, 1.24709e+00, 1.24804e+00, 1.24897e+00, 1.24989e+00, 1.25081e+00], functionval = [0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00  …  0.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, -2.22045e-16, 0.00000e+00, 2.22045e-16, -2.22045e-16, -2.22045e-16, 0.00000e+00], history = [1.54457e+00, 7.94165e-03, 1.55094e-07, 1.88411e-15, 2.67377e-15, 1.70556e-15, 1.58572e-15, 1.81751e-15, 1.76242e-15, 1.77636e-15, 1.76242e-15], stats = (ifun = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], ijac = [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], iarm = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), idid = false, errcode = 10))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heqtest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "heqtest.jl, calls the solver and harvests some iteration statistics. The two columns of numbers are the reults from <cite data-cite=\"chand\"><a href=\"newtonmp.html#chand\">(Cha60)</cite> (page 125). The iteration statistics are from nsold.jl, the solver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NSOL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solver is ```nsol``` from my pacakge [SIAMFANLEquations.jl](#https://github.com/ctkelley/SIAMFANLEquations.jl)\n",
    "<cite data-cite=\"ctk:fajulia\"><a href=\"siamfa.html#ctk:fajulia\">(Kel20d)</cite>. That pacackage and an \n",
    "[IJulia Notebook](#https://github.com/ctkelley/NotebookSIAMFANL) \n",
    "<cite data-cite=\"ctk:notebooknl\"><a href=\"siamfa.html#ctk:notebooknl\">(Kel20b)</cite>    \n",
    "support my upcoming book __Solving Nonlinear Equations with Iterative Methods:\n",
    "Solvers and Examples in Julia__\n",
    "<cite data-cite=\"ctk:siamfanl\"><a href=\"siamfa.html#ctk:siamfanl\">(Kel20c)</cite>\n",
    "The solver and the H-equation example are both from that package. When you run the first code cell you are set up.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using nsol.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the level of this notebook, it's pretty simple. Remember that Julia hates to allocate memory. So your function and Jacobian evaluation routines should expect the calling function to **preallocate** the storage for both the function and Jacobian. Your functions will then use __.=__ to put the function and Jacobian where they are supposed to be.\n",
    "\n",
    "It's worthwhile to look at the help screen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22m \u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22mi \u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ml\u001b[22msc i\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mo\u001b[22mrted tra\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22mc\u001b[0m\u001b[1mo\u001b[22mde tra\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22mp\u001b[0m\u001b[1mo\u001b[22mse Tra\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22mp\u001b[0m\u001b[1mo\u001b[22mse tra\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22mp\u001b[0m\u001b[1mo\u001b[22mse!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "nsol(F!, x0, FS, FPS, J!=diffjac!; rtol=1.e-6, atol=1.e-12,\n",
       "           maxit=20, solver=\"newton\", sham=5, armmax=10, resdec=.1,\n",
       "           dx = 1.e-7, armfix=false, \n",
       "           pdata = nothing, jfact = klfact,\n",
       "           printerr = true, keepsolhist = false, stagnationok=false)\n",
       "\\end{verbatim}\n",
       "C. T. Kelley, 2022\n",
       "\n",
       "Julia versions of the nonlinear solvers from my SIAM books.  Herewith: nsol\n",
       "\n",
       "You must allocate storage for the function and Jacobian in advance –> in the calling program <– ie. in FS and FPS\n",
       "\n",
       "Inputs:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item F!: function evaluation, the ! indicates that F! overwrites FS, your   preallocated storage for the function.\n",
       "\n",
       "So FS=F!(FS,x) or FS=F!(FS,x,pdata) returns FS=F(x)\n",
       "\n",
       "Your function MUST have –> return FS <– at the end.   See the examples in the docstrings and in TestProblems/Systems/simple.jl\n",
       "\n",
       "\n",
       "\\item x0: initial iterate\n",
       "\n",
       "\\end{itemize}\n",
       "\\begin{itemize}\n",
       "\\item FS: Preallocated storage for function. It is a vector of size N\n",
       "\n",
       "You should store it as (N) and design F! to use vectors of size (N).  If you use (N,1) consistently instead, the solvers may work, but I make no guarantees.\n",
       "\n",
       "\n",
       "\\item FPS: preallocated storage for Jacobian. It is an N x N matrix\n",
       "\n",
       "\\end{itemize}\n",
       "\\begin{itemize}\n",
       "\\item J!: Jacobian evaluation, the ! indicates that J! overwrites FPS, your   preallocated storage for the Jacobian. If you leave this out the   default is a finite difference Jacobian.\n",
       "\n",
       "So, FP=J!(FP,FS,x) or FP=J!(FP,FS,x,pdata) returns FP=F'(x). \n",
       "\n",
       "(FP,FS, x) must be the argument list, even if FP does not need FS.   One reason for this is that the finite-difference Jacobian   does and that is the default in the solver.\n",
       "\n",
       "Your Jacobian function MUST have –> return FP <– at the end.   See the examples in the docstrings and in TestProblems/Systems/simple.jl\n",
       "\n",
       "\\end{itemize}\n",
       "\\begin{itemize}\n",
       "\\item Precision: Lemme tell ya 'bout precision. I designed this code for    full precision functions and linear algebra in any precision you want.    You can declare   FPS as Float64, Float32, or Float16 and nsol will do the right thing if   YOU do not destroy the declaration in your J! function. I'm amazed   that this works so easily. If the Jacobian is reasonably well    conditioned, you can cut the cost of Jacobian factorization and   storage in half with no loss. For large dense Jacobians and inexpensive   functions, this is a good deal.\n",
       "\n",
       "BUT ... There is very limited support for direct sparse solvers in   anything other than Float64. I recommend that you only use Float64   with direct sparse solvers unless you really know what you're doing. I   have a couple examples in the notebook, but watch out.\n",
       "\n",
       "\\end{itemize}\n",
       "\\rule{\\textwidth}{1pt}\n",
       "Keyword Arguments (kwargs):\n",
       "\n",
       "rtol and atol: relative and absolute error tolerances\n",
       "\n",
       "maxit: limit on nonlinear iterations\n",
       "\n",
       "solver: default = \"newton\"\n",
       "\n",
       "Your choices are \"newton\" or \"chord\". However, you have sham at your disposal only if you chose newton. \"chord\" will keep using the initial derivative until the iterate converges, uses the iteration budget, or the line search fails. It is not the same as sham=Inf, which is smarter.\n",
       "\n",
       "sham: default = 5 (ie Newton)\n",
       "\n",
       "This is the Shamanskii method. If sham=1, you have Newton. The iteration updates the derivative every sham iterations. The convergence rate has local q-order sham+1 if you only count iterations where you update the derivative. You need not provide your own derivative function to use this option. sham=Inf is chord only if chord is converging well.\n",
       "\n",
       "I made sham=1 the default for scalar equations. For systems I'm more aggressive and want to invest as little energy in linear algebra as possible. So the default is sham=5.\n",
       "\n",
       "armmax: upper bound on step size reductions in line search\n",
       "\n",
       "resdec: default = .1\n",
       "\n",
       "This is the target value for residual reduction. The default value is .1. In the old MATLAB codes it was .5. I only turn Shamanskii on if the residuals are decreasing rapidly, at least a factor of resdec, and the line search is quiescent. If you want to eliminate resdec from the method ( you don't ) then set resdec = 1.0 and you will never hear from it again.\n",
       "\n",
       "dx: default = 1.e-7\n",
       "\n",
       "difference increment in finite-difference derivatives       h=dx*norm(x,Inf)+1.e-8\n",
       "\n",
       "armfix: default = false\n",
       "\n",
       "The default is a parabolic line search (ie false). Set to true and the step size will be fixed at .5. Don't do this unless you are doing experiments for research.\n",
       "\n",
       "pdata:\n",
       "\n",
       "precomputed data for the function/Jacobian.  Things will go better if you use this rather than hide the data  in global variables within the module for your function/Jacobian\n",
       "\n",
       "If you use pdata in either of F! or J!, you must use in in the  calling sequence of both.\n",
       "\n",
       "jfact: default = klfact (tries to figure out best choice) \n",
       "\n",
       "If your Jacobian has any special structure, please set jfact to the correct choice for a factorization.\n",
       "\n",
       "I use jfact when I call PrepareJac! to evaluate the Jacobian (using your J!) and factor it. The default is to use klfact (an internal function) to do something reasonable. For general dense matrices, klfact picks lu! to compute an LU factorization and share storage with the Jacobian.  You may change LU to something else by, for example, setting jfact = cholesky! if your Jacobian is spd.\n",
       "\n",
       "klfact knows about banded matrices and picks qr. You should, however RTFM, allocate the extra two upper bands, and use jfact=qr! to override klfact.\n",
       "\n",
       "klfact uses lu for general sparse matrices.\n",
       "\n",
       "If you give me something that klfact does not know how to dispatch on, then nothing happens. I just return the original Jacobian matrix and  nsol will use backslash to compute the Newton step. I know that this is probably not optimal in your situation, so it is  good to pick something else, like jfact = lu.\n",
       "\n",
       "If you want to manage your own factorization within your Jacobian  evaluation function, then set\n",
       "\n",
       "jfact = nofact\n",
       "\n",
       "and nsol will not attempt to factor your Jacobian. That is also what happens when klfact does not know what to do. Your Jacobian is sent directly to Julia's {\\textbackslash}  operation\n",
       "\n",
       "Please do not mess with the line that calls PrepareJac!. \n",
       "\n",
       "\\begin{verbatim}\n",
       "    FPF = PrepareJac!(FPS, FS, x, ItRules)\n",
       "\\end{verbatim}\n",
       "FPF is not the same as FPS (the storage you allocate for the Jacobian) for a reason. FPF and FPS do not have the same type, even though they share storage. So, FPS=PrepareJac!(FPS, FS, ...) will break things.\n",
       "\n",
       "printerr: default = true\n",
       "\n",
       "I print a helpful message when the solver fails. To suppress that message set printerr to false.\n",
       "\n",
       "keepsolhist: default = false\n",
       "\n",
       "Set this to true to get the history of the iteration in the output tuple. This is on by default for scalar equations and off for systems. Only turn it on if you have use for the data, which can get REALLY LARGE.\n",
       "\n",
       "stagnationok: default = false\n",
       "\n",
       "Set this to true if you want to disable the line search and either observe divergence or stagnation. This is only useful for research or writing a book.\n",
       "\n",
       "Output:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item A named tuple (solution, functionval, history, stats, idid,              errcode, solhist)\n",
       "\n",
       "\\end{itemize}\n",
       "where\n",
       "\n",
       "– solution = converged result\n",
       "\n",
       "– functionval = F(solution)\n",
       "\n",
       "– history = the vector of residual norms (||F(x)||) for the iteration\n",
       "\n",
       "– stats = named tuple of the history of (ifun, ijac, iarm), the number of functions/derivatives/steplength reductions at each iteration.\n",
       "\n",
       "I do not count the function values for a finite-difference derivative because they count toward a Jacobian evaluation. \n",
       "\n",
       "– idid=true if the iteration succeeded and false if not.\n",
       "\n",
       "– errcode = 0 if the iteration succeeded\n",
       "\n",
       "\\begin{verbatim}\n",
       "    = -1 if the initial iterate satisfies the termination criteria\n",
       "\n",
       "    = 10 if no convergence after maxit iterations\n",
       "\n",
       "    = 1  if the line search failed\n",
       "\\end{verbatim}\n",
       "– solhist:\n",
       "\n",
       "This is the entire history of the iteration if you've set keepsolhist=true\n",
       "\n",
       "solhist is an N x K array where N is the length of x and K is the number of iterations + 1. So, for scalar equations, it's a row vector.\n",
       "\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\subsubsection{Examples for nsol}\n",
       "\\paragraph{World's easiest problem example.}\n",
       "Test 64 and 32 bit Jacobians. No meaningful difference in the residual histories or the converged solutions.\n",
       "\n",
       "\\begin{verbatim}\n",
       " julia> function f!(fv,x)\n",
       "       fv[1]=x[1] + sin(x[2])\n",
       "       fv[2]=cos(x[1]+x[2])\n",
       "#\n",
       "# The return fv part is important even though f! overwrites fv.\n",
       "#\n",
       "       return fv\n",
       "       end\n",
       "f (generic function with 1 method)\n",
       "\n",
       "julia> x=ones(2); fv=zeros(2); jv=zeros(2,2); jv32=zeros(Float32,2,2);\n",
       "julia> nout=nsol(f!,x,fv,jv; sham=1);\n",
       "julia> nout32=nsol(f!,x,fv,jv32; sham=1);\n",
       "julia> [nout.history nout32.history]\n",
       "5×2 Matrix{Float64}:\n",
       " 1.88791e+00  1.88791e+00\n",
       " 2.43119e-01  2.43120e-01\n",
       " 1.19231e-02  1.19231e-02\n",
       " 1.03266e-05  1.03265e-05\n",
       " 1.46388e-11  1.45995e-11\n",
       "\n",
       "julia> [nout.solution nout.solution-nout32.solution]\n",
       "2×2 Array{Float64,2}:\n",
       " -7.39085e-01  -5.48450e-14\n",
       "  2.30988e+00  -2.26485e-14\n",
       "\\end{verbatim}\n",
       "\\paragraph{H-equation example.}\n",
       "I'm taking the sham=5 default here, so the convergence is not quadratic. The good news is that we evaluate the Jacobian only once.\n",
       "\n",
       "\\begin{verbatim}\n",
       "julia> n=16; x0=ones(n); FV=ones(n); JV=ones(n,n);\n",
       "julia> hdata=heqinit(x0, .5);\n",
       "julia> hout=nsol(heqf!,x0,FV,JV;pdata=hdata);\n",
       "julia> hout.history\n",
       "4-element Array{Float64,1}:\n",
       " 6.17376e-01\n",
       " 3.17810e-03\n",
       " 2.75227e-05\n",
       " 2.35817e-07\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "nsol(F!, x0, FS, FPS, J!=diffjac!; rtol=1.e-6, atol=1.e-12,\n",
       "           maxit=20, solver=\"newton\", sham=5, armmax=10, resdec=.1,\n",
       "           dx = 1.e-7, armfix=false, \n",
       "           pdata = nothing, jfact = klfact,\n",
       "           printerr = true, keepsolhist = false, stagnationok=false)\n",
       "```\n",
       "\n",
       "C. T. Kelley, 2022\n",
       "\n",
       "Julia versions of the nonlinear solvers from my SIAM books.  Herewith: nsol\n",
       "\n",
       "You must allocate storage for the function and Jacobian in advance –> in the calling program <– ie. in FS and FPS\n",
       "\n",
       "Inputs:\n",
       "\n",
       "  * F!: function evaluation, the ! indicates that F! overwrites FS, your   preallocated storage for the function.\n",
       "\n",
       "    So FS=F!(FS,x) or FS=F!(FS,x,pdata) returns FS=F(x)\n",
       "\n",
       "    Your function MUST have –> return FS <– at the end.   See the examples in the docstrings and in TestProblems/Systems/simple.jl\n",
       "  * x0: initial iterate\n",
       "\n",
       "  * FS: Preallocated storage for function. It is a vector of size N\n",
       "\n",
       "    You should store it as (N) and design F! to use vectors of size (N).  If you use (N,1) consistently instead, the solvers may work, but I make no guarantees.\n",
       "  * FPS: preallocated storage for Jacobian. It is an N x N matrix\n",
       "\n",
       "  * J!: Jacobian evaluation, the ! indicates that J! overwrites FPS, your   preallocated storage for the Jacobian. If you leave this out the   default is a finite difference Jacobian.\n",
       "\n",
       "    So, FP=J!(FP,FS,x) or FP=J!(FP,FS,x,pdata) returns FP=F'(x). \n",
       "\n",
       "    (FP,FS, x) must be the argument list, even if FP does not need FS.   One reason for this is that the finite-difference Jacobian   does and that is the default in the solver.\n",
       "\n",
       "    Your Jacobian function MUST have –> return FP <– at the end.   See the examples in the docstrings and in TestProblems/Systems/simple.jl\n",
       "\n",
       "  * Precision: Lemme tell ya 'bout precision. I designed this code for    full precision functions and linear algebra in any precision you want.    You can declare   FPS as Float64, Float32, or Float16 and nsol will do the right thing if   YOU do not destroy the declaration in your J! function. I'm amazed   that this works so easily. If the Jacobian is reasonably well    conditioned, you can cut the cost of Jacobian factorization and   storage in half with no loss. For large dense Jacobians and inexpensive   functions, this is a good deal.\n",
       "\n",
       "    BUT ... There is very limited support for direct sparse solvers in   anything other than Float64. I recommend that you only use Float64   with direct sparse solvers unless you really know what you're doing. I   have a couple examples in the notebook, but watch out.\n",
       "\n",
       "---\n",
       "\n",
       "Keyword Arguments (kwargs):\n",
       "\n",
       "rtol and atol: relative and absolute error tolerances\n",
       "\n",
       "maxit: limit on nonlinear iterations\n",
       "\n",
       "solver: default = \"newton\"\n",
       "\n",
       "Your choices are \"newton\" or \"chord\". However, you have sham at your disposal only if you chose newton. \"chord\" will keep using the initial derivative until the iterate converges, uses the iteration budget, or the line search fails. It is not the same as sham=Inf, which is smarter.\n",
       "\n",
       "sham: default = 5 (ie Newton)\n",
       "\n",
       "This is the Shamanskii method. If sham=1, you have Newton. The iteration updates the derivative every sham iterations. The convergence rate has local q-order sham+1 if you only count iterations where you update the derivative. You need not provide your own derivative function to use this option. sham=Inf is chord only if chord is converging well.\n",
       "\n",
       "I made sham=1 the default for scalar equations. For systems I'm more aggressive and want to invest as little energy in linear algebra as possible. So the default is sham=5.\n",
       "\n",
       "armmax: upper bound on step size reductions in line search\n",
       "\n",
       "resdec: default = .1\n",
       "\n",
       "This is the target value for residual reduction. The default value is .1. In the old MATLAB codes it was .5. I only turn Shamanskii on if the residuals are decreasing rapidly, at least a factor of resdec, and the line search is quiescent. If you want to eliminate resdec from the method ( you don't ) then set resdec = 1.0 and you will never hear from it again.\n",
       "\n",
       "dx: default = 1.e-7\n",
       "\n",
       "difference increment in finite-difference derivatives       h=dx*norm(x,Inf)+1.e-8\n",
       "\n",
       "armfix: default = false\n",
       "\n",
       "The default is a parabolic line search (ie false). Set to true and the step size will be fixed at .5. Don't do this unless you are doing experiments for research.\n",
       "\n",
       "pdata:\n",
       "\n",
       "precomputed data for the function/Jacobian.  Things will go better if you use this rather than hide the data  in global variables within the module for your function/Jacobian\n",
       "\n",
       "If you use pdata in either of F! or J!, you must use in in the  calling sequence of both.\n",
       "\n",
       "jfact: default = klfact (tries to figure out best choice) \n",
       "\n",
       "If your Jacobian has any special structure, please set jfact to the correct choice for a factorization.\n",
       "\n",
       "I use jfact when I call PrepareJac! to evaluate the Jacobian (using your J!) and factor it. The default is to use klfact (an internal function) to do something reasonable. For general dense matrices, klfact picks lu! to compute an LU factorization and share storage with the Jacobian.  You may change LU to something else by, for example, setting jfact = cholesky! if your Jacobian is spd.\n",
       "\n",
       "klfact knows about banded matrices and picks qr. You should, however RTFM, allocate the extra two upper bands, and use jfact=qr! to override klfact.\n",
       "\n",
       "klfact uses lu for general sparse matrices.\n",
       "\n",
       "If you give me something that klfact does not know how to dispatch on, then nothing happens. I just return the original Jacobian matrix and  nsol will use backslash to compute the Newton step. I know that this is probably not optimal in your situation, so it is  good to pick something else, like jfact = lu.\n",
       "\n",
       "If you want to manage your own factorization within your Jacobian  evaluation function, then set\n",
       "\n",
       "jfact = nofact\n",
       "\n",
       "and nsol will not attempt to factor your Jacobian. That is also what happens when klfact does not know what to do. Your Jacobian is sent directly to Julia's \\  operation\n",
       "\n",
       "Please do not mess with the line that calls PrepareJac!. \n",
       "\n",
       "```\n",
       "    FPF = PrepareJac!(FPS, FS, x, ItRules)\n",
       "```\n",
       "\n",
       "FPF is not the same as FPS (the storage you allocate for the Jacobian) for a reason. FPF and FPS do not have the same type, even though they share storage. So, FPS=PrepareJac!(FPS, FS, ...) will break things.\n",
       "\n",
       "printerr: default = true\n",
       "\n",
       "I print a helpful message when the solver fails. To suppress that message set printerr to false.\n",
       "\n",
       "keepsolhist: default = false\n",
       "\n",
       "Set this to true to get the history of the iteration in the output tuple. This is on by default for scalar equations and off for systems. Only turn it on if you have use for the data, which can get REALLY LARGE.\n",
       "\n",
       "stagnationok: default = false\n",
       "\n",
       "Set this to true if you want to disable the line search and either observe divergence or stagnation. This is only useful for research or writing a book.\n",
       "\n",
       "Output:\n",
       "\n",
       "  * A named tuple (solution, functionval, history, stats, idid,              errcode, solhist)\n",
       "\n",
       "where\n",
       "\n",
       "– solution = converged result\n",
       "\n",
       "– functionval = F(solution)\n",
       "\n",
       "– history = the vector of residual norms (||F(x)||) for the iteration\n",
       "\n",
       "– stats = named tuple of the history of (ifun, ijac, iarm), the number of functions/derivatives/steplength reductions at each iteration.\n",
       "\n",
       "I do not count the function values for a finite-difference derivative because they count toward a Jacobian evaluation. \n",
       "\n",
       "– idid=true if the iteration succeeded and false if not.\n",
       "\n",
       "– errcode = 0 if the iteration succeeded\n",
       "\n",
       "```\n",
       "    = -1 if the initial iterate satisfies the termination criteria\n",
       "\n",
       "    = 10 if no convergence after maxit iterations\n",
       "\n",
       "    = 1  if the line search failed\n",
       "```\n",
       "\n",
       "– solhist:\n",
       "\n",
       "This is the entire history of the iteration if you've set keepsolhist=true\n",
       "\n",
       "solhist is an N x K array where N is the length of x and K is the number of iterations + 1. So, for scalar equations, it's a row vector.\n",
       "\n",
       "---\n",
       "\n",
       "### Examples for nsol\n",
       "\n",
       "#### World's easiest problem example.\n",
       "\n",
       "Test 64 and 32 bit Jacobians. No meaningful difference in the residual histories or the converged solutions.\n",
       "\n",
       "```jldoctest\n",
       " julia> function f!(fv,x)\n",
       "       fv[1]=x[1] + sin(x[2])\n",
       "       fv[2]=cos(x[1]+x[2])\n",
       "#\n",
       "# The return fv part is important even though f! overwrites fv.\n",
       "#\n",
       "       return fv\n",
       "       end\n",
       "f (generic function with 1 method)\n",
       "\n",
       "julia> x=ones(2); fv=zeros(2); jv=zeros(2,2); jv32=zeros(Float32,2,2);\n",
       "julia> nout=nsol(f!,x,fv,jv; sham=1);\n",
       "julia> nout32=nsol(f!,x,fv,jv32; sham=1);\n",
       "julia> [nout.history nout32.history]\n",
       "5×2 Matrix{Float64}:\n",
       " 1.88791e+00  1.88791e+00\n",
       " 2.43119e-01  2.43120e-01\n",
       " 1.19231e-02  1.19231e-02\n",
       " 1.03266e-05  1.03265e-05\n",
       " 1.46388e-11  1.45995e-11\n",
       "\n",
       "julia> [nout.solution nout.solution-nout32.solution]\n",
       "2×2 Array{Float64,2}:\n",
       " -7.39085e-01  -5.48450e-14\n",
       "  2.30988e+00  -2.26485e-14\n",
       "```\n",
       "\n",
       "#### H-equation example.\n",
       "\n",
       "I'm taking the sham=5 default here, so the convergence is not quadratic. The good news is that we evaluate the Jacobian only once.\n",
       "\n",
       "```jldoctest\n",
       "julia> n=16; x0=ones(n); FV=ones(n); JV=ones(n,n);\n",
       "julia> hdata=heqinit(x0, .5);\n",
       "julia> hout=nsol(heqf!,x0,FV,JV;pdata=hdata);\n",
       "julia> hout.history\n",
       "4-element Array{Float64,1}:\n",
       " 6.17376e-01\n",
       " 3.17810e-03\n",
       " 2.75227e-05\n",
       " 2.35817e-07\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  nsol(F!, x0, FS, FPS, J!=diffjac!; rtol=1.e-6, atol=1.e-12,\u001b[39m\n",
       "\u001b[36m             maxit=20, solver=\"newton\", sham=5, armmax=10, resdec=.1,\u001b[39m\n",
       "\u001b[36m             dx = 1.e-7, armfix=false, \u001b[39m\n",
       "\u001b[36m             pdata = nothing, jfact = klfact,\u001b[39m\n",
       "\u001b[36m             printerr = true, keepsolhist = false, stagnationok=false)\u001b[39m\n",
       "\n",
       "  C. T. Kelley, 2022\n",
       "\n",
       "  Julia versions of the nonlinear solvers from my SIAM books. Herewith: nsol\n",
       "\n",
       "  You must allocate storage for the function and Jacobian in advance –> in the\n",
       "  calling program <– ie. in FS and FPS\n",
       "\n",
       "  Inputs:\n",
       "\n",
       "    •  F!: function evaluation, the ! indicates that F! overwrites FS,\n",
       "       your preallocated storage for the function.\n",
       "       So FS=F!(FS,x) or FS=F!(FS,x,pdata) returns FS=F(x)\n",
       "       Your function MUST have –> return FS <– at the end. See the\n",
       "       examples in the docstrings and in TestProblems/Systems/simple.jl\n",
       "\n",
       "    •  x0: initial iterate\n",
       "\n",
       "    •  FS: Preallocated storage for function. It is a vector of size N\n",
       "       You should store it as (N) and design F! to use vectors of size\n",
       "       (N). If you use (N,1) consistently instead, the solvers may work,\n",
       "       but I make no guarantees.\n",
       "\n",
       "    •  FPS: preallocated storage for Jacobian. It is an N x N matrix\n",
       "\n",
       "    •  J!: Jacobian evaluation, the ! indicates that J! overwrites FPS,\n",
       "       your preallocated storage for the Jacobian. If you leave this out\n",
       "       the default is a finite difference Jacobian.\n",
       "       So, FP=J!(FP,FS,x) or FP=J!(FP,FS,x,pdata) returns FP=F'(x).\n",
       "       (FP,FS, x) must be the argument list, even if FP does not need FS.\n",
       "       One reason for this is that the finite-difference Jacobian does\n",
       "       and that is the default in the solver.\n",
       "       Your Jacobian function MUST have –> return FP <– at the end. See\n",
       "       the examples in the docstrings and in\n",
       "       TestProblems/Systems/simple.jl\n",
       "\n",
       "    •  Precision: Lemme tell ya 'bout precision. I designed this code for\n",
       "       full precision functions and linear algebra in any precision you\n",
       "       want. You can declare FPS as Float64, Float32, or Float16 and nsol\n",
       "       will do the right thing if YOU do not destroy the declaration in\n",
       "       your J! function. I'm amazed that this works so easily. If the\n",
       "       Jacobian is reasonably well conditioned, you can cut the cost of\n",
       "       Jacobian factorization and storage in half with no loss. For large\n",
       "       dense Jacobians and inexpensive functions, this is a good deal.\n",
       "       BUT ... There is very limited support for direct sparse solvers in\n",
       "       anything other than Float64. I recommend that you only use Float64\n",
       "       with direct sparse solvers unless you really know what you're\n",
       "       doing. I have a couple examples in the notebook, but watch out.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "  Keyword Arguments (kwargs):\n",
       "\n",
       "  rtol and atol: relative and absolute error tolerances\n",
       "\n",
       "  maxit: limit on nonlinear iterations\n",
       "\n",
       "  solver: default = \"newton\"\n",
       "\n",
       "  Your choices are \"newton\" or \"chord\". However, you have sham at your\n",
       "  disposal only if you chose newton. \"chord\" will keep using the initial\n",
       "  derivative until the iterate converges, uses the iteration budget, or the\n",
       "  line search fails. It is not the same as sham=Inf, which is smarter.\n",
       "\n",
       "  sham: default = 5 (ie Newton)\n",
       "\n",
       "  This is the Shamanskii method. If sham=1, you have Newton. The iteration\n",
       "  updates the derivative every sham iterations. The convergence rate has local\n",
       "  q-order sham+1 if you only count iterations where you update the derivative.\n",
       "  You need not provide your own derivative function to use this option.\n",
       "  sham=Inf is chord only if chord is converging well.\n",
       "\n",
       "  I made sham=1 the default for scalar equations. For systems I'm more\n",
       "  aggressive and want to invest as little energy in linear algebra as\n",
       "  possible. So the default is sham=5.\n",
       "\n",
       "  armmax: upper bound on step size reductions in line search\n",
       "\n",
       "  resdec: default = .1\n",
       "\n",
       "  This is the target value for residual reduction. The default value is .1. In\n",
       "  the old MATLAB codes it was .5. I only turn Shamanskii on if the residuals\n",
       "  are decreasing rapidly, at least a factor of resdec, and the line search is\n",
       "  quiescent. If you want to eliminate resdec from the method ( you don't )\n",
       "  then set resdec = 1.0 and you will never hear from it again.\n",
       "\n",
       "  dx: default = 1.e-7\n",
       "\n",
       "  difference increment in finite-difference derivatives h=dx*norm(x,Inf)+1.e-8\n",
       "\n",
       "  armfix: default = false\n",
       "\n",
       "  The default is a parabolic line search (ie false). Set to true and the step\n",
       "  size will be fixed at .5. Don't do this unless you are doing experiments for\n",
       "  research.\n",
       "\n",
       "  pdata:\n",
       "\n",
       "  precomputed data for the function/Jacobian. Things will go better if you use\n",
       "  this rather than hide the data in global variables within the module for\n",
       "  your function/Jacobian\n",
       "\n",
       "  If you use pdata in either of F! or J!, you must use in in the calling\n",
       "  sequence of both.\n",
       "\n",
       "  jfact: default = klfact (tries to figure out best choice)\n",
       "\n",
       "  If your Jacobian has any special structure, please set jfact to the correct\n",
       "  choice for a factorization.\n",
       "\n",
       "  I use jfact when I call PrepareJac! to evaluate the Jacobian (using your J!)\n",
       "  and factor it. The default is to use klfact (an internal function) to do\n",
       "  something reasonable. For general dense matrices, klfact picks lu! to\n",
       "  compute an LU factorization and share storage with the Jacobian. You may\n",
       "  change LU to something else by, for example, setting jfact = cholesky! if\n",
       "  your Jacobian is spd.\n",
       "\n",
       "  klfact knows about banded matrices and picks qr. You should, however RTFM,\n",
       "  allocate the extra two upper bands, and use jfact=qr! to override klfact.\n",
       "\n",
       "  klfact uses lu for general sparse matrices.\n",
       "\n",
       "  If you give me something that klfact does not know how to dispatch on, then\n",
       "  nothing happens. I just return the original Jacobian matrix and nsol will\n",
       "  use backslash to compute the Newton step. I know that this is probably not\n",
       "  optimal in your situation, so it is good to pick something else, like jfact\n",
       "  = lu.\n",
       "\n",
       "  If you want to manage your own factorization within your Jacobian evaluation\n",
       "  function, then set\n",
       "\n",
       "  jfact = nofact\n",
       "\n",
       "  and nsol will not attempt to factor your Jacobian. That is also what happens\n",
       "  when klfact does not know what to do. Your Jacobian is sent directly to\n",
       "  Julia's \\ operation\n",
       "\n",
       "  Please do not mess with the line that calls PrepareJac!.\n",
       "\n",
       "\u001b[36m      FPF = PrepareJac!(FPS, FS, x, ItRules)\u001b[39m\n",
       "\n",
       "  FPF is not the same as FPS (the storage you allocate for the Jacobian) for a\n",
       "  reason. FPF and FPS do not have the same type, even though they share\n",
       "  storage. So, FPS=PrepareJac!(FPS, FS, ...) will break things.\n",
       "\n",
       "  printerr: default = true\n",
       "\n",
       "  I print a helpful message when the solver fails. To suppress that message\n",
       "  set printerr to false.\n",
       "\n",
       "  keepsolhist: default = false\n",
       "\n",
       "  Set this to true to get the history of the iteration in the output tuple.\n",
       "  This is on by default for scalar equations and off for systems. Only turn it\n",
       "  on if you have use for the data, which can get REALLY LARGE.\n",
       "\n",
       "  stagnationok: default = false\n",
       "\n",
       "  Set this to true if you want to disable the line search and either observe\n",
       "  divergence or stagnation. This is only useful for research or writing a\n",
       "  book.\n",
       "\n",
       "  Output:\n",
       "\n",
       "    •  A named tuple (solution, functionval, history, stats, idid,\n",
       "       errcode, solhist)\n",
       "\n",
       "  where\n",
       "\n",
       "  – solution = converged result\n",
       "\n",
       "  – functionval = F(solution)\n",
       "\n",
       "  – history = the vector of residual norms (||F(x)||) for the iteration\n",
       "\n",
       "  – stats = named tuple of the history of (ifun, ijac, iarm), the number of\n",
       "  functions/derivatives/steplength reductions at each iteration.\n",
       "\n",
       "  I do not count the function values for a finite-difference derivative\n",
       "  because they count toward a Jacobian evaluation.\n",
       "\n",
       "  – idid=true if the iteration succeeded and false if not.\n",
       "\n",
       "  – errcode = 0 if the iteration succeeded\n",
       "\n",
       "\u001b[36m      = -1 if the initial iterate satisfies the termination criteria\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m      = 10 if no convergence after maxit iterations\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m      = 1  if the line search failed\u001b[39m\n",
       "\n",
       "  – solhist:\n",
       "\n",
       "  This is the entire history of the iteration if you've set keepsolhist=true\n",
       "\n",
       "  solhist is an N x K array where N is the length of x and K is the number of\n",
       "  iterations + 1. So, for scalar equations, it's a row vector.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[1m  Examples for nsol\u001b[22m\n",
       "\u001b[1m  –––––––––––––––––––\u001b[22m\n",
       "\n",
       "\u001b[1m  World's easiest problem example.\u001b[22m\n",
       "\u001b[1m  ----------------------------------\u001b[22m\n",
       "\n",
       "  Test 64 and 32 bit Jacobians. No meaningful difference in the residual\n",
       "  histories or the converged solutions.\n",
       "\n",
       "\u001b[36m   julia> function f!(fv,x)\u001b[39m\n",
       "\u001b[36m         fv[1]=x[1] + sin(x[2])\u001b[39m\n",
       "\u001b[36m         fv[2]=cos(x[1]+x[2])\u001b[39m\n",
       "\u001b[36m  #\u001b[39m\n",
       "\u001b[36m  # The return fv part is important even though f! overwrites fv.\u001b[39m\n",
       "\u001b[36m  #\u001b[39m\n",
       "\u001b[36m         return fv\u001b[39m\n",
       "\u001b[36m         end\u001b[39m\n",
       "\u001b[36m  f (generic function with 1 method)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> x=ones(2); fv=zeros(2); jv=zeros(2,2); jv32=zeros(Float32,2,2);\u001b[39m\n",
       "\u001b[36m  julia> nout=nsol(f!,x,fv,jv; sham=1);\u001b[39m\n",
       "\u001b[36m  julia> nout32=nsol(f!,x,fv,jv32; sham=1);\u001b[39m\n",
       "\u001b[36m  julia> [nout.history nout32.history]\u001b[39m\n",
       "\u001b[36m  5×2 Matrix{Float64}:\u001b[39m\n",
       "\u001b[36m   1.88791e+00  1.88791e+00\u001b[39m\n",
       "\u001b[36m   2.43119e-01  2.43120e-01\u001b[39m\n",
       "\u001b[36m   1.19231e-02  1.19231e-02\u001b[39m\n",
       "\u001b[36m   1.03266e-05  1.03265e-05\u001b[39m\n",
       "\u001b[36m   1.46388e-11  1.45995e-11\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> [nout.solution nout.solution-nout32.solution]\u001b[39m\n",
       "\u001b[36m  2×2 Array{Float64,2}:\u001b[39m\n",
       "\u001b[36m   -7.39085e-01  -5.48450e-14\u001b[39m\n",
       "\u001b[36m    2.30988e+00  -2.26485e-14\u001b[39m\n",
       "\n",
       "\u001b[1m  H-equation example.\u001b[22m\n",
       "\u001b[1m  ---------------------\u001b[22m\n",
       "\n",
       "  I'm taking the sham=5 default here, so the convergence is not quadratic. The\n",
       "  good news is that we evaluate the Jacobian only once.\n",
       "\n",
       "\u001b[36m  julia> n=16; x0=ones(n); FV=ones(n); JV=ones(n,n);\u001b[39m\n",
       "\u001b[36m  julia> hdata=heqinit(x0, .5);\u001b[39m\n",
       "\u001b[36m  julia> hout=nsol(heqf!,x0,FV,JV;pdata=hdata);\u001b[39m\n",
       "\u001b[36m  julia> hout.history\u001b[39m\n",
       "\u001b[36m  4-element Array{Float64,1}:\u001b[39m\n",
       "\u001b[36m   6.17376e-01\u001b[39m\n",
       "\u001b[36m   3.17810e-03\u001b[39m\n",
       "\u001b[36m   2.75227e-05\u001b[39m\n",
       "\u001b[36m   2.35817e-07\u001b[39m"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?nsol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How nsol.jl controls the precision of the Jacobian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can control the precision of the Jacobian by simply allocating FPS in your favorite precision. So if I have a problem with N=256 unknowns I will declare FP as zeros(N,1) and may declare FPS as zeros(N,N) or Float32.(zeros(N,N)).\n",
    "\n",
    "Note the __.__ between Float32 and the paren. This, as is standard Julia practice, applies the conversion to Float32 to everyelement in the array. If you forget the __.__ Julia will complain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The results in the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The paper has plots for double, single, and half precsion computations for c=.5, .99, and 1.0. The half precision results take a very long time to get. On my computer (2019 iMac; 8 cores, 64GB of memory) the half precision compute time was over two weeks. Kids, don't try this at home.\n",
    "\n",
    "The data for the paper are in the cleverly named directory __Data_From_Paper__\n",
    "\n",
    "__cd to the directory MPResults__ and __from that directory__ run\n",
    "\n",
    "```Julia\n",
    "data_harvest() \n",
    "```\n",
    "\n",
    "at the julia prompt you will generate all the tables and plots in the paper.\n",
    "\n",
    "If you have the time and patience you can also generate the data with __data_populate.jl__. This creates binary files with the iteration histories and you can see for yourself how long it takes. You can reduce the number of grid levels and __turn half precision off__. I will turn half precision off in the example in this notebook. That means that the code will run in a reasonable amount of time instead of the __two weeks__ it needs for the half precision results.\n",
    "\n",
    "```data_populate(c; half=false,level=p)``` does double and single preicsion solves for $1024 \\times 2^k$ point grids for k=0, ... p-1 . Set ```half=true``` to make the computations take much longer.\n",
    "\n",
    "Here is a simple example of using data_populate and the plotter code plot_nsold.jl. I'm only using the 1024, 2048 and 4096 point grids. The plot in the paper uses more levels. This is part of Figure 1 in the paper.\n",
    "\n",
    "Look at the source to __data_populate.jl__ and __PlotData.jl__ and you'll see how I did this. These codes only mange files, plots, and tables. There is nothing really exciting here. You don't need to know much Julia to understand this, but you do need to know something and I can't help with that.\n",
    "\n",
    "To begin with, I will create a directory called Data_Test to put all this stuff. I will cd to that directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd(MPRdir)\n",
    "Home4mp=\"Data_Test\"\n",
    "try (mkdir(Home4mp))\n",
    "catch\n",
    "end\n",
    "cd(Home4mp)\n",
    "pwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll run __data_populate__ to create a subdirectory with the data. cd to that directory and run __PlotData__. That's how I created the plots in the paper with all values of *c* and all the problem sizes. The half precision computation took two weeks.\n",
    "\n",
    "__PlotData(.5, level=3)__ makes the four-plot with only double and single (no half).\n",
    "\n",
    "Even with this small problem, __data_populate__ takes a while. Be patient. Once it's done the plots will appear\n",
    "pretty rapidly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using PyPlot\n",
    "data_populate(.5; level=3)\n",
    "PlotData(.5; level=3);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I will duplicate the tables and plots in the paper with the precomputed data in the Complete_Data directory. I'll cd to that directory and make the plots with __data_harvest.jl__. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd(MPRdir)\n",
    "cd(\"Complete_Data\")\n",
    "data_harvest() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three LaTeX tables are pretty vivid demonstrations that using a half-precision Jacobian is a poor idea. The columns are ratios of successive residual norms. Those ratios are supposed to go to zero if the convergence is q-superlinear. In the half precision case, it is not. You can also see that from the plots at the bottom. I make the tables with three calls to __MakeTable.jl__ which is contained in the file __TableData.jl__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MakeTable(.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MakeTable(.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MakeTable(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I will cd to the directory that contains the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd(MPRdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
